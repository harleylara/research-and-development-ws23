@inproceedings{referenceexample,
	title = {{Some Title}},
	author = {Mustermann, Max and Smith, Jane},
	booktitle = {Some Conference},
	pages = {1--8},
	year = {2023}
}

@conferencePaper{detryRefiningGraspAffordance2010,
  date = {2010-05-00 05/2010},
  author = {Detry, Renaud and Kraft, Dirk and Buch, Anders Glent and Kruger, Norbert and Piater, Justus},
  libraryCatalog = {DOI.org (Crossref)},
  pages = {2287-2293},
  place = {Anchorage, AK},
  year = {2010},
  title = {Refining grasp affordance models by experience},
  proceedingsTitle = {2010 IEEE International Conference on Robotics and Automation},
  DOI = {10.1109/ROBOT.2010.5509126},
  conferenceName = {2010 IEEE International Conference on Robotics and Automation (ICRA 2010)},
  url = {http://ieeexplore.ieee.org/document/5509126/},
  publisher = {IEEE},
  accessDate = {2023-08-15 21:51:30},
  language = {en},
  ISBN = {978-1-4244-5038-1},
  abstractNote = {We present a method for learning object grasp affordance models in 3D from experience, and demonstrate its applicability through extensive testing and evaluation on a realistic and largely autonomous platform. Grasp affordance refers here to relative object-gripper conﬁgurations that yield stable grasps. These affordances are represented probabilistically with grasp densities, which correspond to continuous density functions deﬁned on the space of 6D gripper poses. A grasp density characterizes an object’s grasp affordance; densities are linked to visual stimuli through registration with a visual model of the object they characterize. We explore a batch-oriented, experience-based learning paradigm where grasps sampled randomly from a density are performed, and an importance-sampling algorithm learns a reﬁned density from the outcomes of these experiences. The ﬁrst such learning cycle is bootstrapped with a grasp density formed from visual cues. We show that the robot effectively applies its experience by downweighting poor grasp solutions, which results in increased success rates at subsequent learning cycles. We also present success rates in a practical scenario where a robot needs to repeatedly grasp an object lying in an arbitrary pose, where each pose imposes a speciﬁc reaching constraint, and thus forces the robot to make use of the entire grasp density to select the most promising achievable grasp.},
}
@preprint{edstedtDeDoDeV2Analyzing2024,
  accessDate = {2024-07-15 04:49:32},
  title = {DeDoDe v2: Analyzing and Improving the DeDoDe Keypoint Detector},
  year = {2024},
  url = {http://arxiv.org/abs/2404.08928},
  date = {2024-04-13 2024-04-13},
  extra = {arXiv:2404.08928 [cs]},
  archiveID = {arXiv:2404.08928},
  language = {en},
  repository = {arXiv},
  shortTitle = {DeDoDe v2},
  abstractNote = {In this paper, we analyze and improve into the recently proposed DeDoDe keypoint detector. We focus our analysis on some key issues. First, we find that DeDoDe keypoints tend to cluster together, which we fix by performing non-max suppression on the target distribution of the detector during training. Second, we address issues related to data augmentation. In particular, the DeDoDe detector is sensitive to large rotations. We fix this by including 90-degree rotations as well as horizontal flips. Finally, the decoupled nature of the DeDoDe detector makes evaluation of downstream usefulness problematic. We fix this by matching the keypoints with a pretrained dense matcher (RoMa) and evaluating two-view pose estimates. We find that the original long training is detrimental to performance, and therefore propose a much shorter training schedule. We integrate all these improvements into our proposed detector DeDoDe v2 and evaluate it with the original DeDoDe descriptor on the MegaDepth-1500 and IMC2022 benchmarks. Our proposed detector significantly increases pose estimation results, notably from 75.9 to 78.3 mAA on the IMC2022 challenge. Code and weights are available at https://github.com/Parskatt/DeDoDe},
  author = {Edstedt, Johan and Bökman, Georg and Zhao, Zhenjun},
  libraryCatalog = {arXiv.org},
}
