
@article{cadena_past_2016,
	title = {Past, {Present}, and {Future} of {Simultaneous} {Localization} and {Mapping}: {Toward} the {Robust}-{Perception} {Age}},
	volume = {32},
	issn = {1552-3098, 1941-0468},
	shorttitle = {Past, {Present}, and {Future} of {Simultaneous} {Localization} and {Mapping}},
	url = {http://ieeexplore.ieee.org/document/7747236/},
	doi = {10.1109/TRO.2016.2624754},
	language = {en},
	number = {6},
	urldate = {2023-07-27},
	journal = {IEEE Transactions on Robotics},
	author = {Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, Jose and Reid, Ian and Leonard, John J.},
	month = dec,
	year = {2016},
	keywords = {start-exploration},
	pages = {1309--1332},
	file = {Cadena et al. - 2016 - Past, Present, and Future of Simultaneous Localiza.pdf:C\:\\Users\\harle\\Zotero\\storage\\IXVZ2WZM\\Cadena et al. - 2016 - Past, Present, and Future of Simultaneous Localiza.pdf:application/pdf},
}

@inproceedings{pham_feature_2016,
	address = {Nadi, Fiji},
	title = {Feature {Descriptors}: {A} {Review} of {Multiple} {Cues} {Approaches}},
	isbn = {978-1-5090-4314-9},
	shorttitle = {Feature {Descriptors}},
	url = {http://ieeexplore.ieee.org/document/7876353/},
	doi = {10.1109/CIT.2016.61},
	abstract = {Feature descriptors have been playing an important role in many computer vision problems, such as image matching and object recognition. While classic descriptors using texture or shape as a single cue of descriptive information have been proved to be successful, recently, several approaches have been proposed introducing the combination of multiple cues to increase descriptive power and robustness. In this paper, we review the methodology of the most recent and popular multiple cues descriptors, and evaluate them with respect to their application and robustness to the variance of conditions.},
	language = {en},
	urldate = {2023-10-13},
	booktitle = {2016 {IEEE} {International} {Conference} on {Computer} and {Information} {Technology} ({CIT})},
	publisher = {IEEE},
	author = {Pham, Cuong H. and Yaguchi, Yuichi and Naruse, Keitaro},
	month = dec,
	year = {2016},
	pages = {310--315},
	file = {Pham et al. - 2016 - Feature Descriptors A Review of Multiple Cues App.pdf:C\:\\Users\\harle\\Zotero\\storage\\5BLLHALS\\Pham et al. - 2016 - Feature Descriptors A Review of Multiple Cues App.pdf:application/pdf},
}

@article{kumar_survey_2014,
	title = {A {Survey} on {Image} {Feature} {Descriptors}},
	volume = {5},
	abstract = {Automatically assigning relevant text keywords to image is an important problem. Many algorithms have been proposed in the past decade and achieved good performance. Efforts have focused upon many other fields but properties of features have not been well investigated. In most cases, a group of features is selected in advance but important feature properties are not well used to feature selection. In this paper the performance of different features are compared, different combinations of features and a number of classification methods applied on the image annotation task, which gives insight into the features properties are also discussed.},
	language = {en},
	author = {Kumar, Rekhil M},
	year = {2014},
	file = {Kumar - 2014 - A Survey on Image Feature Descriptors.pdf:C\:\\Users\\harle\\Zotero\\storage\\ZCF86LFZ\\Kumar - 2014 - A Survey on Image Feature Descriptors.pdf:application/pdf},
}

@article{georgiou_survey_2020,
	title = {A survey of traditional and deep learning-based feature descriptors for high dimensional data in computer vision},
	volume = {9},
	issn = {2192-6611, 2192-662X},
	url = {http://link.springer.com/10.1007/s13735-019-00183-w},
	doi = {10.1007/s13735-019-00183-w},
	abstract = {Higher dimensional data such as video and 3D are the leading edge of multimedia retrieval and computer vision research. In this survey, we give a comprehensive overview and key insights into the state of the art of higher dimensional features from deep learning and also traditional approaches. Current approaches are frequently using 3D information from the sensor or are using 3D in modeling and understanding the 3D world. With the growth of prevalent application areas such as 3D games, self-driving automobiles, health monitoring and sports activity training, a wide variety of new sensors have allowed researchers to develop feature description models beyond 2D. Although higher dimensional data enhance the performance of methods on numerous tasks, they can also introduce new challenges and problems. The higher dimensionality of the data often leads to more complicated structures which present additional problems in both extracting meaningful content and in adapting it for current machine learning algorithms. Due to the major importance of the evaluation process, we also present an overview of the current datasets and benchmarks. Moreover, based on more than 330 papers from this study, we present the major challenges and future directions.},
	language = {en},
	number = {3},
	urldate = {2023-10-13},
	journal = {International Journal of Multimedia Information Retrieval},
	author = {Georgiou, Theodoros and Liu, Yu and Chen, Wei and Lew, Michael},
	month = sep,
	year = {2020},
	pages = {135--170},
	file = {Georgiou et al. - 2020 - A survey of traditional and deep learning-based fe.pdf:C\:\\Users\\harle\\Zotero\\storage\\4C89RCEQ\\Georgiou et al. - 2020 - A survey of traditional and deep learning-based fe.pdf:application/pdf},
}

@article{yudistira_evaluation_nodate,
	title = {Evaluation of {Feature} {Detector}-{Descriptor} for {Real} {Object} {Matching} under {Various} {Conditions} of {Ilumination} and {Affine} {Transformation}},
	abstract = {This study attempts to provide explanations, descriptions and evaluations of some most popular and current combinations of description and descriptor frameworks, namely SIFT, SURF, MSER, and BRISK for keypoint extractors and SIFT, SURF, BRISK, and FREAK for descriptors. Evaluations are made based on the number of matches of keypoints and repeatability in various image variations. It is used as the main parameter to assess how well combinations of algorithms are in matching objects with different variations. There are many papers that describe the comparison of detection and description features to detect objects in images under various conditions, but the combination of algorithms attached to them has not been much discussed. The problem domain is limited to different illumination levels and affine transformations from different perspectives. To evaluate the robustness of all combinations of algorithms, we use a stereo image matching case.},
	language = {en},
	author = {Yudistira, Novanto and Ridok, Achmad and Fauzi, Moch Ali},
	file = {Yudistira et al. - Evaluation of Feature Detector-Descriptor for Real.pdf:C\:\\Users\\harle\\Zotero\\storage\\MFLTRNWP\\Yudistira et al. - Evaluation of Feature Detector-Descriptor for Real.pdf:application/pdf},
}

@article{liu_review_nodate,
	title = {A {Review} of {Image} {Feature} {Descriptors} in {Visual} {Positioning}},
	abstract = {Visual positioning is one of the important research directions of indoor positioning algorithms and systems. Image feature descriptor plays a key role in most visual positioning algorithms, which directly affects the speed and accuracy of positioning. This paper focuses on the application of image feature descriptors in visual positioning and studies image feature detection and extraction algorithms. The image feature descriptors are divided into three categories according to the characteristics of different, namely local gradient-based descriptors, image intensity-based descriptors, and learning-based descriptors. Which steps, characteristics, applicable scene, and application in visual positioning or image matching of the methods are studied. The main purpose of this paper is to make a review of the image feature descriptors that may be used in visual positioning, and provide a reference for the research and innovation of visual indoor positioning using image feature descriptors.},
	language = {en},
	author = {Liu, Wen and Wang, Shuo and Deng, Zhongliang and Chen, Hong},
	file = {Liu et al. - A Review of Image Feature Descriptors in Visual Po.pdf:C\:\\Users\\harle\\Zotero\\storage\\CU5RSEGI\\Liu et al. - A Review of Image Feature Descriptors in Visual Po.pdf:application/pdf},
}

@article{chen_feature_2021,
	title = {Feature detection and description for image matching: from hand-crafted design to deep learning},
	volume = {24},
	issn = {1009-5020, 1993-5153},
	shorttitle = {Feature detection and description for image matching},
	url = {https://www.tandfonline.com/doi/full/10.1080/10095020.2020.1843376},
	doi = {10.1080/10095020.2020.1843376},
	abstract = {In feature based image matching, distinctive features in images are detected and represented by feature descriptors. Matching is then carried out by assessing the similarity of the descrip­ tors of potentially conjugate points. In this paper, we first shortly discuss the general frame­ work. Then, we review feature detection as well as the determination of affine shape and orientation of local features, before analyzing feature description in more detail. In the feature description review, the general framework of local feature description is presented first. Then, the review discusses the evolution from hand-crafted feature descriptors, e.g. SIFT (Scale Invariant Feature Transform), to machine learning and deep learning based descriptors. The machine learning models, the training loss and the respective training data of learning-based algorithms are looked at in more detail; subsequently the various advantages and challenges of the different approaches are discussed. Finally, we present and assess some current research directions before concluding the paper.},
	language = {en},
	number = {1},
	urldate = {2023-10-13},
	journal = {Geo-spatial Information Science},
	author = {Chen, Lin and Rottensteiner, Franz and Heipke, Christian},
	month = jan,
	year = {2021},
	pages = {58--74},
	file = {Chen et al. - 2021 - Feature detection and description for image matchi.pdf:C\:\\Users\\harle\\Zotero\\storage\\XGZNL27G\\Chen et al. - 2021 - Feature detection and description for image matchi.pdf:application/pdf},
}

@article{leng_local_2019,
	title = {Local {Feature} {Descriptor} for {Image} {Matching}: {A} {Survey}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {Local {Feature} {Descriptor} for {Image} {Matching}},
	url = {https://ieeexplore.ieee.org/document/8584423/},
	doi = {10.1109/ACCESS.2018.2888856},
	abstract = {Image registration is an important technique in many computer vision applications such as image fusion, image retrieval, object tracking, face recognition, change detection and so on. Local feature descriptors, i.e., how to detect features and how to describe them, play a fundamental and important role in image registration process, which directly inﬂuence the accuracy and robustness of image registration. This paper mainly focuses on the variety of local feature descriptors including some theoretical research, mathematical models, and methods or algorithms along with their applications in the context of image registration. The existing local feature descriptors are roughly classiﬁed into six categories to demonstrate and analyze comprehensively their own advantages. The current and future challenges of local feature descriptors are discussed. The major goal of the paper is to present a unique survey of the state-of-the-art image matching methods based on feature descriptor, from which future research may beneﬁt.},
	language = {en},
	urldate = {2023-10-13},
	journal = {IEEE Access},
	author = {Leng, Chengcai and Zhang, Hai and Li, Bo and Cai, Guorong and Pei, Zhao and He, Li},
	year = {2019},
	pages = {6424--6434},
	file = {Leng et al. - 2019 - Local Feature Descriptor for Image Matching A Sur.pdf:C\:\\Users\\harle\\Zotero\\storage\\8SBAA8MD\\Leng et al. - 2019 - Local Feature Descriptor for Image Matching A Sur.pdf:application/pdf},
}

@misc{lindenberger_lightglue_2023,
	title = {{LightGlue}: {Local} {Feature} {Matching} at {Light} {Speed}},
	shorttitle = {{LightGlue}},
	url = {http://arxiv.org/abs/2306.13643},
	abstract = {We introduce LightGlue, a deep neural network that learns to match local features across images. We revisit multiple design decisions of SuperGlue, the state of the art in sparse matching, and derive simple but effective improvements. Cumulatively, they make LightGlue more efficient – in terms of both memory and computation, more accurate, and much easier to train. One key property is that LightGlue is adaptive to the difficulty of the problem: the inference is much faster on image pairs that are intuitively easy to match, for example because of a larger visual overlap or limited appearance change. This opens up exciting prospects for deploying deep matchers in latency-sensitive applications like 3D reconstruction. The code and trained models are publicly available at github.com/cvg/LightGlue.},
	language = {en},
	urldate = {2023-10-18},
	publisher = {arXiv},
	author = {Lindenberger, Philipp and Sarlin, Paul-Edouard and Pollefeys, Marc},
	month = jun,
	year = {2023},
	note = {arXiv:2306.13643 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Lindenberger et al. - 2023 - LightGlue Local Feature Matching at Light Speed.pdf:C\:\\Users\\harle\\Zotero\\storage\\H87YANHU\\Lindenberger et al. - 2023 - LightGlue Local Feature Matching at Light Speed.pdf:application/pdf},
}

@article{mikolajczyk_performance_2005,
	title = {A {Performance} {Evaluation} of {Local} {Descriptors}},
	volume = {27},
	abstract = {In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [32]. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [3], steerable filters [12], PCA-SIFT [19], differential invariants [20], spin images [21], SIFT [26], complex filters [37], moment invariants [43], and cross-correlation for different types of interest regions. We also propose an extension of the SIFT descriptor and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors.},
	language = {en},
	number = {10},
	journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE},
	author = {Mikolajczyk, Krystian and Schmid, Cordelia},
	year = {2005},
	file = {Mikolajczyk and Schmid - 2005 - A Performance Evaluation of Local Descriptors.pdf:C\:\\Users\\harle\\Zotero\\storage\\F4TNC99E\\Mikolajczyk and Schmid - 2005 - A Performance Evaluation of Local Descriptors.pdf:application/pdf},
}

@article{kummerle_measuring_2009,
	title = {On measuring the accuracy of {SLAM} algorithms},
	volume = {27},
	issn = {0929-5593, 1573-7527},
	url = {http://link.springer.com/10.1007/s10514-009-9155-6},
	doi = {10.1007/s10514-009-9155-6},
	abstract = {In this paper, we address the problem of creating an objective benchmark for evaluating SLAM approaches. We propose a framework for analyzing the results of a SLAM approach based on a metric for measuring the error of the corrected trajectory. This metric uses only relative relations between poses and does not rely on a global reference frame. This overcomes serious shortcomings of approaches using a global reference frame to compute the error. Our method furthermore allows us to compare SLAM approaches that use different estimation techniques or different sensor modalities since all computations are made based on the corrected trajectory of the robot.},
	language = {en},
	number = {4},
	urldate = {2023-10-26},
	journal = {Autonomous Robots},
	author = {Kümmerle, Rainer and Steder, Bastian and Dornhege, Christian and Ruhnke, Michael and Grisetti, Giorgio and Stachniss, Cyrill and Kleiner, Alexander},
	month = nov,
	year = {2009},
	pages = {387--407},
	file = {Kümmerle et al. - 2009 - On measuring the accuracy of SLAM algorithms.pdf:C\:\\Users\\harle\\Zotero\\storage\\MQ3UDD23\\Kümmerle et al. - 2009 - On measuring the accuracy of SLAM algorithms.pdf:application/pdf},
}

@article{bhargava_towards_2021,
	title = {Towards {Development} of {Performance} {Metrics} for {Benchmarking} {SLAM} {Algorithms}},
	volume = {1964},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1964/6/062115},
	doi = {10.1088/1742-6596/1964/6/062115},
	abstract = {The true autonomy of mobile robots cannot be achieved without Simultaneous Localization and Mapping (SLAM). With this capability, mobile robot could concurrently build a map of the environment and locate itself with respect to the map. Although there are several variants of SLAM algorithms contributed by researchers so far, only a very few works were aimed at comparing their performances with appropriate metrics and providing detailed directions and insights to the user on selection criteria and indicative use cases. In this work, we presented a comparative study of three popular SLAM algorithms and provide some significant quantitative performance measures of the same by using our novel {\textbar} R {\textbar} and {\textbar} S {\textbar} performance metrics as well as conventional metrics. The comparative study was carried out in ROS (Robot Operating System) using Turtlebot3 robot model on three SLAM packages viz Gmapping, Karto SLAM, and Frontier Exploration SLAM. Furthermore, the results show that the proposed metrics are very efficient and compact in comparing and quantifying the performance of SLAM algorithms.},
	language = {en},
	number = {6},
	urldate = {2023-10-26},
	journal = {Journal of Physics: Conference Series},
	author = {Bhargava, Mudit and Mehta, Rushad and Adhikari, Chandan Das and Sivanathan, K},
	month = jul,
	year = {2021},
	pages = {062115},
	file = {Bhargava et al. - 2021 - Towards Development of Performance Metrics for Ben.pdf:C\:\\Users\\harle\\Zotero\\storage\\PQP96J2Z\\Bhargava et al. - 2021 - Towards Development of Performance Metrics for Ben.pdf:application/pdf},
}

@inproceedings{prokhorov_measuring_2019,
	address = {Tokyo, Japan},
	title = {Measuring robustness of {Visual} {SLAM}},
	isbn = {978-4-901122-18-4},
	url = {https://ieeexplore.ieee.org/document/8758020/},
	doi = {10.23919/MVA.2019.8758020},
	abstract = {Simultaneous localisation and mapping (SLAM) is an essential component of robotic systems. In this work we perform a feasibility study of RGB-D SLAM for the task of indoor robot navigation. Recent visual SLAM methods, e.g. ORBSLAM2 [9], demonstrate really impressive accuracy, but the experiments in the papers are usually conducted on just a few sequences, that makes it diﬃcult to reason about the robustness of the methods. Another problem is that all available RGBD datasets contain the trajectories with very complex camera motions. In this work we extensively evaluate ORBSLAM2 to better understand the state-of-the-art. First, we conduct experiments on the popular publicly available datasets for RGB-D SLAM across the conventional metrics. We perform statistical analysis of the results and ﬁnd correlations between the metrics and the attributes of the trajectories. Then, we introduce a new large and diverse HomeRobot dataset where we model the motions of a simple home robot. Our dataset is created using physically-based rendering with realistic lighting and contains the scenes composed by human designers. It includes thousands of sequences, that is two orders of magnitude greater than in previous works. We ﬁnd that while in many cases the accuracy of SLAM is very good, the robustness is still an issue.},
	language = {en},
	urldate = {2023-10-26},
	booktitle = {2019 16th {International} {Conference} on {Machine} {Vision} {Applications} ({MVA})},
	publisher = {IEEE},
	author = {Prokhorov, David and Zhukov, Dmitry and Barinova, Olga and Anton, Konushin and Vorontsova, Anna},
	month = may,
	year = {2019},
	pages = {1--6},
	file = {Prokhorov et al. - 2019 - Measuring robustness of Visual SLAM.pdf:C\:\\Users\\harle\\Zotero\\storage\\AVDYKZX8\\Prokhorov et al. - 2019 - Measuring robustness of Visual SLAM.pdf:application/pdf},
}

@misc{bujanca_robust_2021,
	title = {Robust {SLAM} {Systems}: {Are} {We} {There} {Yet}?},
	shorttitle = {Robust {SLAM} {Systems}},
	url = {http://arxiv.org/abs/2109.13160},
	abstract = {Progress in the last decade has brought about signiﬁcant improvements in the accuracy and speed of SLAM systems, broadening their mapping capabilities. Despite these advancements, long-term operation remains a major challenge, primarily due to the wide spectrum of perturbations robotic systems may encounter.},
	language = {en},
	urldate = {2023-10-26},
	publisher = {arXiv},
	author = {Bujanca, Mihai and Shi, Xuesong and Spear, Matthew and Zhao, Pengpeng and Lennox, Barry and Lujan, Mikel},
	month = sep,
	year = {2021},
	note = {arXiv:2109.13160 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Bujanca et al. - 2021 - Robust SLAM Systems Are We There Yet.pdf:C\:\\Users\\harle\\Zotero\\storage\\KELBZI6K\\Bujanca et al. - 2021 - Robust SLAM Systems Are We There Yet.pdf:application/pdf},
}

@article{guo_comprehensive_2016,
	title = {A {Comprehensive} {Performance} {Evaluation} of {3D} {Local} {Feature} {Descriptors}},
	volume = {116},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-015-0824-y},
	doi = {10.1007/s11263-015-0824-y},
	abstract = {A number of 3D local feature descriptors have been proposed in the literature. It is however, unclear which descriptors are more appropriate for a particular application. A good descriptor should be descriptive, compact, and robust to a set of nuisances. This paper compares ten popular local feature descriptors in the contexts of 3D object recognition, 3D shape retrieval, and 3D modeling. We ﬁrst evaluate the descriptiveness of these descriptors on eight popular datasets which were acquired using different techniques. We then analyze their compactness using the recall of feature matching per each ﬂoat value in the descriptor. We also test the robustness of the selected descriptors with respect to support radius variations, Gaussian noise, shot noise, varying mesh resolution, distance to the mesh boundary, keypoint localization error, occlusion, clutter, and dataset size. Moreover, we present the performance results of these descriptors when combined with different 3D keypoint detection methods. We ﬁnally analyze the computational efﬁciency for generating each descriptor.},
	language = {en},
	number = {1},
	urldate = {2023-11-14},
	journal = {International Journal of Computer Vision},
	author = {Guo, Yulan and Bennamoun, Mohammed and Sohel, Ferdous and Lu, Min and Wan, Jianwei and Kwok, Ngai Ming},
	month = jan,
	year = {2016},
	pages = {66--89},
	file = {Guo et al. - 2016 - A Comprehensive Performance Evaluation of 3D Local.pdf:C\:\\Users\\harle\\Zotero\\storage\\3A5DKZCC\\Guo et al. - 2016 - A Comprehensive Performance Evaluation of 3D Local.pdf:application/pdf},
}

@article{bayraktar_analysis_2017,
	title = {Analysis of feature detector and descriptor combinations with a localization experiment for various performance metrics},
	volume = {25},
	issn = {13000632, 13036203},
	url = {https://journals.tubitak.gov.tr/elektrik/vol25/iss3/66},
	doi = {10.3906/elk-1602-225},
	abstract = {The purpose of this study is to give a detailed performance comparison about the feature detector and descriptor methods, particularly when their various combinations are used for image matching. As the case study, the localization experiments of a mobile robot in an indoor environment are given. In these experiments, 3090 query images and 127 dataset images are used. This study includes five methods for feature detectors such as features from accelerated segment test (FAST), oriented FAST and rotated binary robust independent elementary features (BRIEF) (ORB), speeded-up robust features (SURF), scale invariant feature transform (SIFT), binary robust invariant scalable keypoints (BRISK), and five other methods for feature descriptors which are BRIEF, BRISK, SIFT, SURF, and ORB. These methods are used in 23 different combinations and it was possible to obtain meaningful and consistent comparison results using some performance criteria defined in this study. All of these methods are used independently and separately from each other as being feature detector or descriptor. The performance analysis shows the discriminative power of various combinations of detector and descriptor methods. The analysis is completed using five parameters such as (i) accuracy, (ii) time, (iii) angle difference between keypoints, (iv) number of correct matches, and (v) distance between correctly matched keypoints. In a range of 60°, covering five rotational pose points for our system, “FAST-SURF” combination gave the best results with the lowest distance and angle difference values and highest number of matched keypoints. The combination “SIFT-SURF” is obtained as the most accurate combination with 98.41\% of correct classification rate. The fastest algorithm is achieved with “ORB-BRIEF” combination with a total running time 21303.30 seconds in order to match 560 images captured during the motion with 127 dataset images.},
	language = {en},
	urldate = {2023-11-14},
	journal = {TURKISH JOURNAL OF ELECTRICAL ENGINEERING \& COMPUTER SCIENCES},
	author = {Bayraktar, Ertuğrul and Boyraz, Pınar},
	year = {2017},
	pages = {2444--2454},
	file = {Bayraktar and Boyraz - 2017 - Analysis of feature detector and descriptor combin.pdf:C\:\\Users\\harle\\Zotero\\storage\\KUVFSMSF\\Bayraktar and Boyraz - 2017 - Analysis of feature detector and descriptor combin.pdf:application/pdf},
}

@article{madbouly_performance_2015,
	title = {Performance {Assessment} of {Feature} {Detector}-{Descriptor} {Combination}},
	volume = {12},
	abstract = {Features detection and description among multiple images are widely used in many applications, e.g., feature matching, object categorization, 3D construction, image retrieval and object recognition. This paper evaluates combination performance of different feature detectors and descriptors. It will compare performance of detectors and descriptors combination on images under rotate, scale constraints and distortion such as illumination on different scene (bedroom, industrial and CALsuburb). An experimental result shows MinEigen detector has best result in number of detected key-points when handle rotate, scale and illumination and not affected with scene. SURF without external detector is the best when handle rotate and scale constraint in different levels and scene. FAST/SURF and Harris/FREAK are best combined against illumination distortion in different levels. This review introduces a brief introduction for providing a new research in feature detection field to find appropriate method according to their condition.},
	language = {en},
	number = {5},
	journal = {International Journal of Computer Science Issues},
	author = {Madbouly, A M M and Wafy, M and Mostafa, Mostafa-Sami M},
	year = {2015},
	file = {Madbouly et al. - 2015 - Performance Assessment of Feature Detector-Descrip.pdf:C\:\\Users\\harle\\Zotero\\storage\\5HIGDRRK\\Madbouly et al. - 2015 - Performance Assessment of Feature Detector-Descrip.pdf:application/pdf},
}

@book{cremers_computer_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Computer {Vision} -- {ACCV} 2014: 12th {Asian} {Conference} on {Computer} {Vision}, {Singapore}, {Singapore}, {November} 1-5, 2014, {Revised} {Selected} {Papers}, {Part} {II}},
	volume = {9004},
	isbn = {978-3-319-16807-4 978-3-319-16808-1},
	shorttitle = {Computer {Vision} -- {ACCV} 2014},
	url = {https://link.springer.com/10.1007/978-3-319-16808-1},
	language = {en},
	urldate = {2023-11-14},
	publisher = {Springer International Publishing},
	editor = {Cremers, Daniel and Reid, Ian and Saito, Hideo and Yang, Ming-Hsuan},
	year = {2015},
	doi = {10.1007/978-3-319-16808-1},
	file = {Cremers et al. - 2015 - Computer Vision -- ACCV 2014 12th Asian Conferenc.pdf:C\:\\Users\\harle\\Zotero\\storage\\KX8T6IJB\\Cremers et al. - 2015 - Computer Vision -- ACCV 2014 12th Asian Conferenc.pdf:application/pdf},
}

@article{mikolajczyk_performance_2005-1,
	title = {A performance evaluation of local descriptors},
	volume = {27},
	issn = {0162-8828},
	url = {http://ieeexplore.ieee.org/document/1498756/},
	doi = {10.1109/TPAMI.2005.188},
	abstract = {In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [32]. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [3], steerable filters [12], PCA-SIFT [19], differential invariants [20], spin images [21], SIFT [26], complex filters [37], moment invariants [43], and cross-correlation for different types of interest regions. We also propose an extension of the SIFT descriptor and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors.},
	language = {en},
	number = {10},
	urldate = {2023-11-14},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Mikolajczyk, K. and Schmid, C.},
	month = oct,
	year = {2005},
	pages = {1615--1630},
	file = {Mikolajczyk and Schmid - 2005 - A performance evaluation of local descriptors.pdf:C\:\\Users\\harle\\Zotero\\storage\\FAULBFUB\\Mikolajczyk and Schmid - 2005 - A performance evaluation of local descriptors.pdf:application/pdf},
}

@article{miksik_evaluation_2012,
	title = {Evaluation of local detectors and descriptors for fast feature matching},
	abstract = {Local feature detectors and descriptors are widely used in many computer vision applications and various methods have been proposed during the past decade. There have been a number of evaluations focused on various aspects of local features, matching accuracy in particular, however there has been no comparisons considering the accuracy and speed trade-offs of recent extractors such as BRIEF, BRISK, ORB, MRRID, MROGH and LIOP. This paper provides a performance evaluation of recent feature detectors and compares their matching precision and speed in randomized kdtrees setup as well as an evaluation of binary descriptors with efﬁcient computation of Hamming distance.},
	language = {en},
	journal = {Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)},
	author = {Miksik, Ondrej and Mikolajczyk, Krystian},
	year = {2012},
	pages = {2681--2684},
	file = {Miksik and Mikolajczyk - Evaluation of local detectors and descriptors for .pdf:C\:\\Users\\harle\\Zotero\\storage\\JCF3HFY5\\Miksik and Mikolajczyk - Evaluation of local detectors and descriptors for .pdf:application/pdf},
}

@inproceedings{chandrasekhar_feature_2014,
	address = {Snowbird, UT, USA},
	title = {Feature {Matching} {Performance} of {Compact} {Descriptors} for {Visual} {Search}},
	isbn = {978-1-4799-3882-7},
	url = {http://ieeexplore.ieee.org/document/6824408/},
	doi = {10.1109/DCC.2014.50},
	abstract = {MPEG is currently developing a standard titled Compact Descriptors for Visual Search (CDVS) for descriptor extraction and compression. In this work, we report comprehensive patch-level experiments for a direct comparison of low bitrate descriptors for visual search. For evaluating different compression schemes, we propose a data set of matching pairs of image patches from the MPEG-CDVS image-level data sets. We propose a greedy rate allocation scheme for distributing bits across different spatial bins of the SIFT descriptor. We study a scheme based on Entropy Constrained Vector Quantization and greedy rate allocation, which performs close to the performance bound for any compression scheme. Finally, we present extensive feature-level Receiver Operating Characteristic (ROC) comparisons for different compression schemes (Vector Quantization, Transform Coding, Lattice Coding) proposed during the MPEG-CDVS standardization process.},
	language = {en},
	urldate = {2023-11-14},
	booktitle = {2014 {Data} {Compression} {Conference}},
	publisher = {IEEE},
	author = {Chandrasekhar, Vijay and Takacs, Gabriel and Chen, David M. and Tsai, Sam S. and Makar, Mina and Girod, Bernd},
	month = mar,
	year = {2014},
	pages = {3--12},
	file = {Chandrasekhar et al. - 2014 - Feature Matching Performance of Compact Descriptor.pdf:C\:\\Users\\harle\\Zotero\\storage\\HG3BGMDS\\Chandrasekhar et al. - 2014 - Feature Matching Performance of Compact Descriptor.pdf:application/pdf},
}

@inproceedings{tola_fast_2008,
	address = {Anchorage, AK, USA},
	title = {A fast local descriptor for dense matching},
	isbn = {978-1-4244-2242-5},
	url = {http://ieeexplore.ieee.org/document/4587673/},
	doi = {10.1109/CVPR.2008.4587673},
	abstract = {We introduce a novel local image descriptor designed for dense wide-baseline matching purposes. We feed our descriptors to a graph-cuts based dense depth map estimation algorithm and this yields better wide-baseline performance than the commonly used correlation windows for which the size is hard to tune. As a result, unlike competing techniques that require many high-resolution images to produce good reconstructions, our descriptor can compute them from pairs of low-quality images such as the ones captured by video streams.},
	language = {en},
	urldate = {2023-11-14},
	booktitle = {2008 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Tola, Engin and Lepetit, Vincent and Fua, Pascal},
	month = jun,
	year = {2008},
	pages = {1--8},
	file = {Tola et al. - 2008 - A fast local descriptor for dense matching.pdf:C\:\\Users\\harle\\Zotero\\storage\\HYH2PHYR\\Tola et al. - 2008 - A fast local descriptor for dense matching.pdf:application/pdf},
}

@article{li_review_2017,
	title = {A review of feature detection and match algorithms for localization and mapping},
	volume = {231},
	issn = {1757-8981, 1757-899X},
	url = {https://iopscience.iop.org/article/10.1088/1757-899X/231/1/012003},
	doi = {10.1088/1757-899X/231/1/012003},
	abstract = {Localization and mapping is an essential ability of a robot to keep track of its own location in an unknown environment. Among existing methods for this purpose, vision-based methods are more effective solutions for being accurate, inexpensive and versatile. Visionbased methods can generally be categorized as feature-based approaches and appearance-based approaches. The feature-based approaches prove higher performance in textured scenarios. However, their performance depend highly on the applied feature-detection algorithms. In this paper, we surveyed algorithms for feature detection, which is an essential step in achieving vision-based localization and mapping. In this pater, we present mathematical models of the algorithms one after another. To compare the performances of the algorithms, we conducted a series of experiments on their accuracy, speed, scale invariance and rotation invariance. The results of the experiments showed that ORB is the fastest algorithm in detecting and matching features, the speed of which is more than 10 times that of SURF and approximately 40 times that of SIFT. And SIFT, although with no advantage in terms of speed, shows the most correct matching pairs and proves its accuracy.},
	language = {en},
	urldate = {2023-11-14},
	journal = {IOP Conference Series: Materials Science and Engineering},
	author = {Li, Shimiao},
	month = sep,
	year = {2017},
	pages = {012003},
	file = {Li - 2017 - A review of feature detection and match algorithms.pdf:C\:\\Users\\harle\\Zotero\\storage\\N2PIG68D\\Li - 2017 - A review of feature detection and match algorithms.pdf:application/pdf},
}

@article{karami_image_nodate,
	title = {Image {Matching} {Using} {SIFT}, {SURF}, {BRIEF} and {ORB}: {Performance} {Comparison} for {Distorted} {Images}},
	abstract = {Fast and robust image matching is a very important task with various applications in computer vision and robotics. In this paper, we compare the performance of three different image matching techniques, i.e., SIFT, SURF, and ORB, against different kinds of transformations and deformations such as scaling, rotation, noise, fish eye distortion, and shearing. For this purpose, we manually apply different types of transformations on original images and compute the matching evaluation parameters such as the number of key points in images, the matching rate, and the execution time required for each algorithm and we will show that which algorithm is the best more robust against each kind of distortion.},
	language = {en},
	author = {Karami, Ebrahim and Prasad, Siva and Shehata, Mohamed},
	file = {Karami et al. - Image Matching Using SIFT, SURF, BRIEF and ORB Pe.pdf:C\:\\Users\\harle\\Zotero\\storage\\4U233IFS\\Karami et al. - Image Matching Using SIFT, SURF, BRIEF and ORB Pe.pdf:application/pdf},
}

@inproceedings{dahl_finding_2011,
	address = {Hangzhou, TBD, China},
	title = {Finding the {Best} {Feature} {Detector}-{Descriptor} {Combination}},
	isbn = {978-1-61284-429-9},
	url = {http://ieeexplore.ieee.org/document/5955377/},
	doi = {10.1109/3DIMPVT.2011.47},
	abstract = {Addressing the image correspondence problem by feature matching is a central part of computer vision and 3D inference from images. Consequently, there is a substantial amount of work on evaluating feature detection and feature description methodology. However, the performance of the feature matching is an interplay of both detector and descriptor methodology. Our main contribution is to evaluate the performance of some of the most popular descriptor and detector combinations on the DTU Robot dataset, which is a very large dataset with massive amounts of systematic data aimed at two view matching. The size of the dataset implies that we can also reasonably make deductions about the statistical signiﬁcance of our results. We conclude, that the MSER and Difference of Gaussian (DoG) detectors with a SIFT or DAISY descriptor are the top performers. This performance is, however, not statistically signiﬁcantly better than some other methods. As a byproduct of this investigation, we have also tested various DAISY type descriptors, and found that the difference among their performance is statistically insigniﬁcant using this dataset. Furthermore, we have not been able to produce results collaborating that using afﬁne invariant feature detectors carries a statistical signiﬁcant advantage on general scene types.},
	language = {en},
	urldate = {2023-11-14},
	booktitle = {2011 {International} {Conference} on {3D} {Imaging}, {Modeling}, {Processing}, {Visualization} and {Transmission}},
	publisher = {IEEE},
	author = {Dahl, Anders Lindbjerg and Aanæs, Henrik and Pedersen, Kim Steenstrup},
	month = may,
	year = {2011},
	pages = {318--325},
	file = {Dahl et al. - 2011 - Finding the Best Feature Detector-Descriptor Combi.pdf:C\:\\Users\\harle\\Zotero\\storage\\B6CYFXW6\\Dahl et al. - 2011 - Finding the Best Feature Detector-Descriptor Combi.pdf:application/pdf},
}

@incollection{hassaballah_analysis_2019,
	address = {Cham},
	title = {Analysis and {Evaluation} of {Keypoint} {Descriptors} for {Image} {Matching}},
	volume = {804},
	isbn = {978-3-030-02999-9 978-3-030-03000-1},
	url = {http://link.springer.com/10.1007/978-3-030-03000-1_5},
	abstract = {Feature keypoint descriptors have become indispensable tools and have been widely utilized in a large number of computer vision applications. Many descriptors have been proposed in the literature to describe regions of interest around each keypoint and each claims distinctiveness and robustness against certain types of image distortions. Among these are the conventional ﬂoating-point descriptors and their binary competitors that require less storage capacity and perform at a fraction of the matching times compared with the ﬂoating-point descriptors. This chapter gives a brief description to the most frequently used keypoint descriptors from each category. Also, it provides a general framework to analyze and evaluate the performance of these feature keypoint descriptors, particularly when they are used for image matching under various imaging distortions such as blur, scale and illumination changes, and image rotations. Moreover, it presents a detailed explanation and analysis of the experimental results and ﬁndings where several important observations are derived from the conducted experiments.},
	language = {en},
	urldate = {2023-11-14},
	booktitle = {Recent {Advances} in {Computer} {Vision}},
	publisher = {Springer International Publishing},
	author = {Hassaballah, M. and Alshazly, Hammam A. and Ali, Abdelmgeid A.},
	editor = {Hassaballah, Mahmoud and Hosny, Khalid M.},
	year = {2019},
	doi = {10.1007/978-3-030-03000-1_5},
	note = {Series Title: Studies in Computational Intelligence},
	pages = {113--140},
	file = {Hassaballah et al. - 2019 - Analysis and Evaluation of Keypoint Descriptors fo.pdf:C\:\\Users\\harle\\Zotero\\storage\\26R9YM5Y\\Hassaballah et al. - 2019 - Analysis and Evaluation of Keypoint Descriptors fo.pdf:application/pdf},
}

@inproceedings{kadir_features_2013,
	address = {Penang, Malaysia},
	title = {Features detection and matching for visual simultaneous localization and mapping ({VSLAM})},
	isbn = {978-1-4799-1508-8 978-1-4799-1506-4},
	url = {http://ieeexplore.ieee.org/document/6719929/},
	doi = {10.1109/ICCSCE.2013.6719929},
	abstract = {This paper presents the feature detection method for aerial image. The image captured from the navigation was used to select the best landmarks for localization and mapping in SLAM. A robust visual detection method has contributed to better landmark and data association selection. Therefore, different feature detection algorithms were compared to evaluate the best landmark detector and descriptor for the VSLAM. The performances of the feature detectors were evaluated using dataset provided by the Robotics Research Group at University of Oxford. The local images of matching effect on the detector and descriptor have proved the correctness of key point matching. The selected method has been validated and proven efficient for the VSLAM.},
	language = {en},
	urldate = {2023-11-14},
	booktitle = {2013 {IEEE} {International} {Conference} on {Control} {System}, {Computing} and {Engineering}},
	publisher = {IEEE},
	author = {Kadir, Herdawatie Abdul and Arshad, Mohd Rizal},
	month = nov,
	year = {2013},
	pages = {40--45},
	file = {Kadir and Arshad - 2013 - Features detection and matching for visual simulta.pdf:C\:\\Users\\harle\\Zotero\\storage\\3DA977GF\\Kadir and Arshad - 2013 - Features detection and matching for visual simulta.pdf:application/pdf},
}

@incollection{kamel_comparison_2015,
	address = {Cham},
	title = {A {Comparison} of {Feature} {Detectors} and {Descriptors} in {RGB}-{D} {SLAM} {Methods}},
	volume = {9164},
	isbn = {978-3-319-20800-8 978-3-319-20801-5},
	url = {https://link.springer.com/10.1007/978-3-319-20801-5_32},
	abstract = {In RGB-D based SLAM methods, robot motion is generally computed by detecting and matching feature points in image frames obtained from an RGB-D sensor. Thus, feature detectors and descriptors used in a SLAM method signiﬁcantly aﬀect the performance. In this work, impacts of feature detectors and descriptors on the performance of an RGB-D based SLAM method are studied. SIFT, SURF, BRISK, ORB, FAST, GFTT, STAR feature detectors and SIFT, SURF, BRISK, ORB, BRIEF, FREAK feature descriptors are evaluated in terms of accuracy and speed.},
	language = {en},
	urldate = {2023-11-14},
	booktitle = {Image {Analysis} and {Recognition}},
	publisher = {Springer International Publishing},
	author = {Guclu, Oguzhan and Can, Ahmet Burak},
	editor = {Kamel, Mohamed and Campilho, Aurélio},
	year = {2015},
	doi = {10.1007/978-3-319-20801-5_32},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {297--305},
	file = {Guclu and Can - 2015 - A Comparison of Feature Detectors and Descriptors .pdf:C\:\\Users\\harle\\Zotero\\storage\\SCDSS2CH\\Guclu and Can - 2015 - A Comparison of Feature Detectors and Descriptors .pdf:application/pdf},
}

@inproceedings{hartmann_comparison_2013,
	address = {Barcelona, Catalonia, Spain},
	title = {A comparison of feature descriptors for visual {SLAM}},
	isbn = {978-1-4799-0263-7},
	url = {http://ieeexplore.ieee.org/document/6698820/},
	doi = {10.1109/ECMR.2013.6698820},
	abstract = {Feature detection and feature description plays an important part in Visual Simultaneous Localization and Mapping (VSLAM). Visual features are commonly used to efﬁciently estimate the motion of the camera (visual odometry) and link the current image to previously visited parts of the environment (place recognition, loop closure). Gradient histogram-based feature descriptors, like SIFT and SURF, are frequently used for this task. Recently introduced binary descriptors, as BRIEF or BRISK, claim to offer similar capabilities at lower computational cost. In this paper, we will compare the most popular feature descriptors in a typical graph-based VSLAM algorithm using two publicly available datasets to determine the impact of the choice for feature descriptor in terms of accuracy and speed in a realistic scenario.},
	language = {en},
	urldate = {2023-11-14},
	booktitle = {2013 {European} {Conference} on {Mobile} {Robots}},
	publisher = {IEEE},
	author = {Hartmann, Jan and Klussendorff, Jan Helge and Maehle, Erik},
	month = sep,
	year = {2013},
	pages = {56--61},
	file = {Hartmann et al. - 2013 - A comparison of feature descriptors for visual SLA.pdf:C\:\\Users\\harle\\Zotero\\storage\\28FYVYHG\\Hartmann et al. - 2013 - A comparison of feature descriptors for visual SLA.pdf:application/pdf},
}

@misc{sarlin_superglue_2020,
	title = {{SuperGlue}: {Learning} {Feature} {Matching} with {Graph} {Neural} {Networks}},
	shorttitle = {{SuperGlue}},
	url = {http://arxiv.org/abs/1911.11763},
	abstract = {This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly ﬁnding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a ﬂexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at github.com/magicleap/SuperGluePretrainedNetwork.},
	language = {en},
	urldate = {2023-11-14},
	publisher = {arXiv},
	author = {Sarlin, Paul-Edouard and DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
	month = mar,
	year = {2020},
	note = {arXiv:1911.11763 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Sarlin et al. - 2020 - SuperGlue Learning Feature Matching with Graph Ne.pdf:C\:\\Users\\harle\\Zotero\\storage\\DLMKFKGZ\\Sarlin et al. - 2020 - SuperGlue Learning Feature Matching with Graph Ne.pdf:application/pdf},
}

@article{bailey_simultaneous_nodate,
	title = {Simultaneous {Localisation} and {Mapping} ({SLAM}): {Part} {II} {State} of the {Art}},
	abstract = {This tutorial provides an introduction to the Simultaneous Localisation and Mapping (SLAM) method and the extensive research on SLAM that has been undertaken. Part I of this tutorial described the essential SLAM problem. Part II of this tutorial (this paper) is concerned with recent advances in computational methods and in new formulations of the SLAM problem for large scale and complex environments.},
	language = {en},
	author = {Bailey, Tim and Durrant-Whyte, Hugh},
	file = {Bailey and Durrant-Whyte - Simultaneous Localisation and Mapping (SLAM) Part.pdf:C\:\\Users\\harle\\Zotero\\storage\\TNGZUV5V\\Bailey and Durrant-Whyte - Simultaneous Localisation and Mapping (SLAM) Part.pdf:application/pdf},
}

@article{tourani_visual_2022,
	title = {Visual {SLAM}: {What} are the {Current} {Trends} and {What} to {Expect}?},
	volume = {22},
	issn = {1424-8220},
	shorttitle = {Visual {SLAM}},
	url = {http://arxiv.org/abs/2210.10491},
	doi = {10.3390/s22239297},
	abstract = {Vision-based sensors have shown signiﬁcant performance, accuracy, and efﬁciency gain in Simultaneous Localization and Mapping (SLAM) systems in recent years. In this regard, Visual Simultaneous Localization and Mapping (VSLAM) methods refer to the SLAM approaches that employ cameras for pose estimation and map generation. We can see many research works that demonstrated VSLAMs can outperform traditional methods, which rely only on a particular sensor, such as a Lidar, even with lower costs. VSLAM approaches utilize different camera types (e.g., monocular, stereo, and RGB-D), have been tested on various datasets (e.g., KITTI, TUM RGB-D, and EuRoC) and in dissimilar environments (e.g., indoors and outdoors), and employ multiple algorithms and methodologies to have a better understanding of the environment. The mentioned variations have made this topic popular for researchers and resulted in a wide range of VSLAMs methodologies. In this regard, the primary intent of this survey is to present the recent advances in VSLAM systems, along with discussing the existing challenges and trends. We have given an in-depth literature survey of fortyﬁve impactful papers published in the domain of VSLAMs. We have classiﬁed these manuscripts by different characteristics, including the novelty domain, objectives, employed algorithms, and semantic level. We also discuss the current trends and future directions that may help researchers investigate them.},
	language = {en},
	number = {23},
	urldate = {2023-11-25},
	journal = {Sensors},
	author = {Tourani, Ali and Bavle, Hriday and Sanchez-Lopez, Jose Luis and Voos, Holger},
	month = nov,
	year = {2022},
	note = {arXiv:2210.10491 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, I.2.9, I.4.9},
	pages = {9297},
	file = {Tourani et al. - 2022 - Visual SLAM What are the Current Trends and What .pdf:C\:\\Users\\harle\\Zotero\\storage\\AKGK3RQN\\Tourani et al. - 2022 - Visual SLAM What are the Current Trends and What .pdf:application/pdf},
}

@article{durrant-whyte_simultaneous_nodate,
	title = {Simultaneous {Localisation} and {Mapping} ({SLAM}): {Part} {I} {The} {Essential} {Algorithms}},
	abstract = {This tutorial provides an introduction to Simultaneous Localisation and Mapping (SLAM) and the extensive research on SLAM that has been undertaken over the past decade. SLAM is the process by which a mobile robot can build a map of an environment and at the same time use this map to compute it’s own location. The past decade has seen rapid and exciting progress in solving the SLAM problem together with many compelling implementations of SLAM methods. Part I of this tutorial (this paper), describes the probabilistic form of the SLAM problem, essential solution methods and signiﬁcant implementations. Part II of this tutorial will be concerned with recent advances in computational methods and new formulations of the SLAM problem for large scale and complex environments.},
	language = {en},
	author = {Durrant-Whyte, Hugh and Bailey, Tim},
	file = {Durrant-Whyte and Bailey - Simultaneous Localisation and Mapping (SLAM) Part.pdf:C\:\\Users\\harle\\Zotero\\storage\\4P6JPKM6\\Durrant-Whyte and Bailey - Simultaneous Localisation and Mapping (SLAM) Part.pdf:application/pdf},
}
